{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a7a6f3",
   "metadata": {},
   "source": [
    "<h1>Maritime: Weather</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccbe392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import atan2, degrees\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "# PATHS\n",
    "AIS_COLLISION_CSV = \"collision/ais raw data/gfw_encounters_recent.csv\"\n",
    "AIS_WEATHER_CSV = \"weather/ais_weather.csv\"\n",
    "AIS_WEATHERAPI_CSV = \"weather/ais_weatherapi.csv\"\n",
    "\n",
    "WIND_NC = \"weather/wind10m.nc\"\n",
    "WAVES_NC = \"weather/waves_subset.nc\"\n",
    "CURRENTS_NC = \"weather/currents_subset.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdc34c",
   "metadata": {},
   "source": [
    "<h2>Retrieving Raw AIS Data Used In Collision to call Weather API to get data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603052e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ts_utc    lat     lon  sog cog      mmsi vessel_type\n",
      "2025-03-21T05:00:00Z 36.947 123.323  1.3     100040506     fishing\n",
      "2025-03-19T15:20:00Z 30.824 123.253  1.9 039 100106189     fishing\n",
      "2025-03-21T04:20:00Z 30.849 123.276  1.9     100106189     fishing\n",
      "2025-03-20T14:40:00Z 28.315 123.851  0.0     100222222     fishing\n",
      "2025-03-20T10:40:00Z 22.356 117.581  0.0 281 100305237     fishing\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(AIS_COLLISION_CSV)\n",
    "\n",
    "# --- choose timestamp ---\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# --- lat/lon (prefer explicit columns; fallback to boundingBox) ---\n",
    "def parse_bbox(b):\n",
    "    # expected like \"[lat1, lon1, lat2, lon2]\" or \"[lon, lat, lon, lat]\" depending on source!\n",
    "    try:\n",
    "        vals = [float(x) for x in str(b).strip(\"[]\").split(\",\")]\n",
    "        if len(vals) >= 2:\n",
    "            # Heuristic: if |vals[0]| <= 90 and |vals[1]| <= 180, assume [lat, lon]\n",
    "            if abs(vals[0]) <= 90 and abs(vals[1]) <= 180:\n",
    "                return pd.Series({\"lat_fallback\": vals[0], \"lon_fallback\": vals[1]})\n",
    "            else:\n",
    "                # else assume [lon, lat]\n",
    "                return pd.Series({\"lat_fallback\": vals[1], \"lon_fallback\": vals[0]})\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.Series({\"lat_fallback\": np.nan, \"lon_fallback\": np.nan})\n",
    "\n",
    "bbox_fallback = df[\"boundingBox\"].apply(parse_bbox)\n",
    "df = pd.concat([df, bbox_fallback], axis=1)\n",
    "\n",
    "df[\"lat\"] = df[\"position__lat\"].where(df[\"position__lat\"].notna(), df[\"lat_fallback\"])\n",
    "df[\"lon\"] = df[\"position__lon\"].where(df[\"position__lon\"].notna(), df[\"lon_fallback\"])\n",
    "\n",
    "# --- sog ---\n",
    "df[\"sog\"] = pd.to_numeric(df[\"encounter__medianSpeedKnots\"], errors=\"coerce\").round(1)\n",
    "\n",
    "# --- mmsi from ssvid (keep only 9-digit numeric if you want strict MMSI) ---\n",
    "def normalize_mmsi(x):\n",
    "    s = str(x).strip()\n",
    "    return s if s.isdigit() and len(s) == 9 else np.nan\n",
    "\n",
    "df[\"mmsi\"] = df[\"vessel__ssvid\"].apply(normalize_mmsi)\n",
    "\n",
    "# --- vessel_type ---\n",
    "df[\"vessel_type\"] = df[\"vessel__type\"].fillna(\"Unknown\")\n",
    "\n",
    "# --- compute COG by bearing to next point per vessel ---\n",
    "# Sort by vessel + time\n",
    "df[\"_t\"] = pd.to_datetime(df[\"ts_utc\"], errors=\"coerce\")\n",
    "df = df.sort_values([\"mmsi\", \"_t\"])\n",
    "\n",
    "# Bearing helper\n",
    "def bearing_deg(lat1, lon1, lat2, lon2):\n",
    "    if any(pd.isna([lat1, lon1, lat2, lon2])):\n",
    "        return np.nan\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    y = np.sin(dlon) * np.cos(lat2)\n",
    "    x = np.cos(lat1)*np.sin(lat2) - np.sin(lat1)*np.cos(lat2)*np.cos(dlon)\n",
    "    brng = (degrees(atan2(y, x)) + 360) % 360\n",
    "    return brng\n",
    "\n",
    "# Next point within same vessel\n",
    "df[\"lat_next\"] = df.groupby(\"mmsi\")[\"lat\"].shift(-1)\n",
    "df[\"lon_next\"] = df.groupby(\"mmsi\")[\"lon\"].shift(-1)\n",
    "\n",
    "df[\"cog\"] = df.apply(lambda r: bearing_deg(r[\"lat\"], r[\"lon\"], r[\"lat_next\"], r[\"lon_next\"]), axis=1)\n",
    "df[\"cog\"] = df[\"cog\"].round(0).astype(\"Int64\").astype(str).str.zfill(3)\n",
    "df.loc[df[\"cog\"]==\"<NA>\", \"cog\"] = \"\"\n",
    "\n",
    "# --- final selection & cleaning ---\n",
    "out = df[[\"ts_utc\", \"lat\", \"lon\", \"sog\", \"cog\", \"mmsi\", \"vessel_type\"]].copy()\n",
    "out = out.dropna(subset=[\"ts_utc\", \"lat\", \"lon\"])\n",
    "out[\"lat\"] = out[\"lat\"].round(3)\n",
    "out[\"lon\"] = out[\"lon\"].round(3)\n",
    "\n",
    "# save\n",
    "out.to_csv(AIS_WEATHER_CSV, index=False)\n",
    "print(out.head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d08dd",
   "metadata": {},
   "source": [
    "<h2>Calling copernicusmarine API and wind.f000 to get relevant weather data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcaf286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_vars_to_points(nc_path, ais_df, wanted):\n",
    "    \"\"\"\n",
    "    Samples variables from an xarray dataset at AIS (time, lat, lon) points.\n",
    "    - Works with 1-D or 2-D lat/lon.\n",
    "    - Uses time selection only if there is a real time dimension; otherwise skips time.\n",
    "    \"\"\"\n",
    "    if not nc_path or nc_path == \"-\" or not Path(nc_path).exists():\n",
    "        return {}\n",
    "\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "\n",
    "    # ---- find lat/lon names ----\n",
    "    lat_name = next((n for n in [\"latitude\",\"lat\",\"nav_lat\"] if n in ds.variables or n in ds.coords), None)\n",
    "    lon_name = next((n for n in [\"longitude\",\"lon\",\"nav_lon\"] if n in ds.variables or n in ds.coords), None)\n",
    "    if not lat_name or not lon_name:\n",
    "        raise SystemExit(f\"Could not find lat/lon in coords={list(ds.coords)} vars={list(ds.data_vars)}\")\n",
    "\n",
    "    # ---- collapse 2-D lat/lon (y,x) to 1-D if separable ----\n",
    "    latn, lonn = None, None\n",
    "    if getattr(ds[lat_name], \"ndim\", 1) == 2 and \"y\" in ds.dims and \"x\" in ds.dims:\n",
    "        lat2d = ds[lat_name].values\n",
    "        lon2d = ds[lon_name].values\n",
    "        lat1d = lat2d[:, 0]\n",
    "        lon1d = lon2d[0, :]\n",
    "        if np.allclose(lat2d, lat1d[:, None], equal_nan=True) and np.allclose(lon2d, lon1d[None, :], equal_nan=True):\n",
    "            ds = ds.assign_coords(y=(\"y\", lat1d), x=(\"x\", lon1d))\n",
    "            latn, lonn = \"y\", \"x\"\n",
    "        else:\n",
    "            raise SystemExit(\n",
    "                \"Field has non-separable 2D lat/lon. Re-export as regular 1D latitude/longitude NetCDF.\"\n",
    "            )\n",
    "\n",
    "    # Pick 1-D coord names (normal case or after collapsing)\n",
    "    def pick1d(ds, cands):\n",
    "        for c in cands:\n",
    "            if (c in ds.coords or c in ds.dims) and getattr(ds[c], \"ndim\", 1) == 1:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    if latn is None: latn = pick1d(ds, [\"latitude\",\"lat\",\"y\"])\n",
    "    if lonn is None: lonn = pick1d(ds, [\"longitude\",\"lon\",\"x\"])\n",
    "    if latn is None or lonn is None:\n",
    "        raise SystemExit(f\"No 1-D lat/lon coords. Coords: {list(ds.coords)} | Dims: {list(ds.dims)}\")\n",
    "\n",
    "    # ---- time handling ----\n",
    "    tdim = None\n",
    "    for cand in (\"time\",\"valid_time\",\"step\"):\n",
    "        if cand in ds.dims and getattr(ds[cand], \"ndim\", 1) == 1:\n",
    "            tdim = cand\n",
    "            break\n",
    "\n",
    "    # If only one time slice, drop time dim\n",
    "    if tdim and ds.dims.get(tdim, 0) == 1:\n",
    "        ds = ds.isel({tdim: 0})\n",
    "        tdim = None\n",
    "\n",
    "    # keep only variables we actually have\n",
    "    avail = [v for v in wanted if v in ds.variables]\n",
    "    if not avail:\n",
    "        return {}\n",
    "\n",
    "    # sort coords if 1-D\n",
    "    for c in ([tdim] if tdim else []) + [latn, lonn]:\n",
    "        if c and c in ds.coords and getattr(ds[c], \"ndim\", 1) == 1:\n",
    "            ds = ds.sortby(c)\n",
    "\n",
    "    # build selection target\n",
    "    target = {\n",
    "        latn: xr.DataArray(ais_df[\"lat\"].values, dims=\"points\"),\n",
    "        lonn: xr.DataArray(ais_df[\"lon\"].values, dims=\"points\"),\n",
    "    }\n",
    "    if tdim:\n",
    "        target[tdim] = xr.DataArray(ais_df[\"ts_utc\"].values, dims=\"points\")\n",
    "\n",
    "    sampled = ds[avail].sel(target, method=\"nearest\")\n",
    "    return {v: sampled[v].values for v in avail}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a05e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load AIS FROM AIS_WEATHER_CSV and normalize ===\n",
    "\n",
    "# Expecting columns at least: ts_utc, lat, lon\n",
    "# Optional but recommended: sog, cog (degrees 0..359)\n",
    "ais = pd.read_csv(AIS_WEATHER_CSV)\n",
    "\n",
    "# Ensure columns exist or map common variants\n",
    "rename_map = {}\n",
    "if \"timestamp\" in ais.columns: rename_map[\"timestamp\"] = \"ts_utc\"\n",
    "if \"Latitude\"  in ais.columns: rename_map[\"Latitude\"]  = \"lat\"\n",
    "if \"Longitude\" in ais.columns: rename_map[\"Longitude\"] = \"lon\"\n",
    "if \"SOG\"       in ais.columns: rename_map[\"SOG\"]       = \"sog\"\n",
    "if \"COG\"       in ais.columns: rename_map[\"COG\"]       = \"cog\"\n",
    "ais = ais.rename(columns=rename_map)\n",
    "\n",
    "# Required columns check\n",
    "for col in (\"ts_utc\",\"lat\",\"lon\"):\n",
    "    if col not in ais.columns:\n",
    "        raise SystemExit(f\"Missing AIS column: {col}\")\n",
    "\n",
    "# Parse time -> naive UTC (ns precision ok for xarray)\n",
    "ais[\"ts_utc\"] = pd.to_datetime(ais[\"ts_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "# Coerce numerics and clean\n",
    "for c in (\"lat\",\"lon\",\"sog\"):\n",
    "    if c in ais.columns:\n",
    "        ais[c] = pd.to_numeric(ais[c], errors=\"coerce\")\n",
    "\n",
    "# COG: keep as numeric degrees for trig later\n",
    "if \"cog\" in ais.columns:\n",
    "    ais[\"cog\"] = pd.to_numeric(ais[\"cog\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c50c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauxi\\AppData\\Local\\Temp\\ipykernel_31104\\2802190509.py:53: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if tdim and ds.dims.get(tdim, 0) == 1:\n",
      "<frozen _collections_abc>:774: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "C:\\Users\\lauxi\\AppData\\Local\\Temp\\ipykernel_31104\\2802190509.py:53: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if tdim and ds.dims.get(tdim, 0) == 1:\n",
      "<frozen _collections_abc>:774: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    }
   ],
   "source": [
    "# === Sample environmental fields ===\n",
    "\n",
    "waves = sample_vars_to_points(WAVES_NC,    ais, wanted=[\"VHM0\",\"VMDR\",\"VTM10\",\"VTPK\"])\n",
    "currs = sample_vars_to_points(CURRENTS_NC, ais, wanted=[\"uo\",\"vo\",\"eastward_current\",\"northward_current\"])\n",
    "wind  = sample_vars_to_points(WIND_NC,     ais, wanted=[\"u10\",\"v10\",\"UGRD_10maboveground\",\"VGRD_10maboveground\"])\n",
    "\n",
    "out = ais.copy()\n",
    "\n",
    "# Attach samples if present\n",
    "for k, v in waves.items(): out[k] = v\n",
    "for k, v in currs.items(): out[k] = v\n",
    "for k, v in wind.items():  out[k] = v\n",
    "\n",
    "# Standardize aliases\n",
    "if \"UGRD_10maboveground\" in out.columns: out = out.rename(columns={\"UGRD_10maboveground\":\"u10\"})\n",
    "if \"VGRD_10maboveground\" in out.columns: out = out.rename(columns={\"VGRD_10maboveground\":\"v10\"})\n",
    "if \"eastward_current\"    in out.columns: out = out.rename(columns={\"eastward_current\":\"uo\"})\n",
    "if \"northward_current\"   in out.columns: out = out.rename(columns={\"northward_current\":\"vo\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23180231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Derived features ===\n",
    "\n",
    "# Waves\n",
    "if \"VHM0\" in out.columns:\n",
    "    out[\"stormy\"] = (out[\"VHM0\"] >= 3.0).astype(int)\n",
    "\n",
    "# Relative wave angle / sector (needs VMDR and COG)\n",
    "if {\"VMDR\",\"cog\"}.issubset(out.columns):\n",
    "    # VMDR = mean wave direction (from-direction). Convert to “to” direction to compare with COG.\n",
    "    wave_to = (out[\"VMDR\"] + 180) % 360\n",
    "    ang = np.abs((wave_to - out[\"cog\"]) % 360)\n",
    "    ang = np.where(ang > 180, 360 - ang, ang)\n",
    "    out[\"rel_wave_angle\"] = ang\n",
    "    out[\"sea_sector\"] = pd.cut(\n",
    "        out[\"rel_wave_angle\"],\n",
    "        bins=[-0.1, 30, 150, 180],\n",
    "        labels=[\"following\", \"beam\", \"head\"]\n",
    "    )\n",
    "\n",
    "# Currents projection into along/cross relative to COG\n",
    "if {\"uo\",\"vo\",\"cog\"}.issubset(out.columns):\n",
    "    theta = np.deg2rad(out[\"cog\"])\n",
    "    hx, hy = np.sin(theta), np.cos(theta)  # heading unit vector (x=east, y=north)\n",
    "    along_ms = out[\"uo\"] * hx + out[\"vo\"] * hy\n",
    "    cross_ms = -out[\"uo\"] * hy + out[\"vo\"] * hx\n",
    "    out[\"along_current_kn\"] = along_ms * 1.94384\n",
    "    out[\"cross_current_kn\"] = cross_ms * 1.94384\n",
    "    if \"sog\" in out.columns:\n",
    "        out[\"stw_est_kn\"] = out[\"sog\"] - out[\"along_current_kn\"]  # crude STW estimate\n",
    "\n",
    "# Wind speed/dir and relative wind angle\n",
    "if {\"u10\",\"v10\"}.issubset(out.columns):\n",
    "    out[\"wind_speed_ms\"] = np.hypot(out[\"u10\"], out[\"v10\"])\n",
    "    # “wind-to” direction (0=N, 90=E) using arctan2(u_east, v_north)\n",
    "    out[\"wind_dir_to\"] = (np.degrees(np.arctan2(out[\"u10\"], out[\"v10\"])) + 360) % 360\n",
    "    if \"cog\" in out.columns:\n",
    "        wang = (out[\"wind_dir_to\"] - out[\"cog\"]) % 360\n",
    "        out[\"rel_wind_angle\"] = np.where(wang > 180, 360 - wang, wang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "527869a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote weather/ais_weatherapi.csv with shape (8000, 23)\n"
     ]
    }
   ],
   "source": [
    "# === Save to your desired output ===\n",
    "out.to_csv(AIS_WEATHERAPI_CSV, index=False)\n",
    "print(\"Wrote\", AIS_WEATHERAPI_CSV, \"with shape\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9821c",
   "metadata": {},
   "source": [
    "<h2>Checking new csv file</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54388aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 23) ['ts_utc', 'lat', 'lon', 'sog', 'cog', 'mmsi', 'vessel_type', 'VHM0', 'VMDR', 'VTM10', 'uo', 'vo', 'u10', 'v10', 'stormy', 'rel_wave_angle', 'sea_sector', 'along_current_kn', 'cross_current_kn', 'stw_est_kn', 'wind_speed_ms', 'wind_dir_to', 'rel_wind_angle']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stw_est_kn          0.791500\n",
       "cross_current_kn    0.791500\n",
       "along_current_kn    0.791500\n",
       "sea_sector          0.791500\n",
       "rel_wave_angle      0.791500\n",
       "vo                  0.468250\n",
       "uo                  0.468250\n",
       "VHM0                0.468250\n",
       "VTM10               0.468250\n",
       "VMDR                0.468000\n",
       "rel_wind_angle      0.360000\n",
       "cog                 0.360000\n",
       "mmsi                0.018375\n",
       "lat                 0.000000\n",
       "u10                 0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(AIS_WEATHERAPI_CSV)\n",
    "\n",
    "print(df.shape, df.columns.tolist())\n",
    "df.describe(include='all').T.head(20)\n",
    "\n",
    "# essential fields present\n",
    "assert {\"ts_utc\",\"lat\",\"lon\"}.issubset(df.columns)\n",
    "# lat/lon sanity\n",
    "assert df[\"lat\"].between(-90,90).all()\n",
    "assert df[\"lon\"].between(-180,180).all()\n",
    "df.isna().mean().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d226ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time and sort\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df.sort_values([\"ts_utc\",\"lat\",\"lon\"]).drop_duplicates(subset=[\"ts_utc\",\"lat\",\"lon\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
